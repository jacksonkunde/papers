### "AI, Don't Be Sexist."
  
  This article from Anthropic AI outlined their new constitutional AI system that they used to train their LLM “Claude.” Their approach builds off the approach of Reinforcement Learning with Human Feedback, which has been used to train many LLMs such as GPT4. However, they replace human feedback with more AI. On the one hand, this approach limits the “human toll” of training these models. To train LLMs using RLHF, companies like OpenAI pay workers in developing countries pennies to comb through the gruesome outputs of these LLMs. On the other hand, this approach feels like a never-ending cycle of AI, begging the question: “When it will stop?” Is the next step to build an AI that generates the constitution, to give to the AI which enforces the constitution on the LLM? Learning about constitutional AI reinforces the importance of mechanistic interpretability research. Constitutional AI seems like a lazy and unfortunately necessary solution because we don’t actually understand why LLMs produce the response they do. Reading this article, I am left wondering if the solution to bias in AI systems has been staring at us all along. Is it as simple as saying “Hey AI, stop being racist”? It seems too good to be true.

  Beyond the AI system, the work Anthropic is doing to outline how AI should operate through this constitution is important, regardless of how the constitution is being used in constitutional AI. I am relieved that researchers are sitting down and being specific about how AI should act. Though, I am worried about how the constitution may develop over time. Anthropic writes in the article that “…if the model displays some behavior you don’t like, you can typically try to write a principle to discourage it.” I will be interested to see if the constitution slowly grows over time, to catch the many edge cases, or if it remains pithy.
